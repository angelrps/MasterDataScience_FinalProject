{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Premilinary Works\n",
    "How big is the file, number of rows, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/angelrps/git/MasterDataScience_FinalProject/data\n"
     ]
    }
   ],
   "source": [
    "cd ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\r\n",
      "total 2897760\r\n",
      "-rwxrwxrwx 1 angelrps angelrps 1048588401 Mar 25 13:15 2017_Yellow_Taxi_Trip_Data.csv.bz2\r\n",
      "-rwxrwxrwx 1 angelrps angelrps 1065918551 Jun 18 12:58 2018_Yellow_Taxi_Trip_Data.csv.bz2\r\n",
      "-rwxrwxrwx 1 angelrps angelrps  835929368 Apr  2 18:02 2019_Yellow_Taxi_Trip_Data.csv.bz2\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     564780 Jun 18 11:15 Data_Weather_Cleaned.csv\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     436485 Jun 16 10:51 LCD_documentation.pdf\r\n",
      "-rwxrwxrwx 1 angelrps angelrps   13843633 Jun 16 09:40 NOAA_CentralPark_Weather.csv\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     196848 Apr  7 19:34 data_dictionary_trip_records_yellow.pdf\r\n",
      "-rwxrwxrwx 1 angelrps angelrps    1489310 Apr  7 19:34 taxi_zone_map_manhattan.jpg\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     202694 Apr  4 12:46 trip_record_user_guide.pdf\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rl\n",
    "# 2017_Yellow_Taxi_Trip_Data.csv is 10532305815 bytes (9,80 GB)\n",
    "# 2018_Yellow_Taxi_Trip_Data.csv is 10428263736 bytes (9,71 GB)\n",
    "# 2019_Yellow_Taxi_Trip_Data.csv is 8197837930 bytes (7,63 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am dealing with very heavy files so I will compress them (bz2) to work with them\n",
    "# and remove the heavy .csv from my drive\n",
    "!bzip2 2017_Yellow_Taxi_Trip_Data.csv 2018_Yellow_Taxi_Trip_Data.csv 2019_Yellow_Taxi_Trip_Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\r\n",
      "total 2897760\r\n",
      "-rwxrwxrwx 1 angelrps angelrps 1048588401 Mar 25 13:15 2017_Yellow_Taxi_Trip_Data.csv.bz2\r\n",
      "-rwxrwxrwx 1 angelrps angelrps 1065918551 Jun 18 12:58 2018_Yellow_Taxi_Trip_Data.csv.bz2\r\n",
      "-rwxrwxrwx 1 angelrps angelrps  835929368 Apr  2 18:02 2019_Yellow_Taxi_Trip_Data.csv.bz2\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     564780 Jun 18 11:15 Data_Weather_Cleaned.csv\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     436485 Jun 16 10:51 LCD_documentation.pdf\r\n",
      "-rwxrwxrwx 1 angelrps angelrps   13843633 Jun 16 09:40 NOAA_CentralPark_Weather.csv\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     196848 Apr  7 19:34 data_dictionary_trip_records_yellow.pdf\r\n",
      "-rwxrwxrwx 1 angelrps angelrps    1489310 Apr  7 19:34 taxi_zone_map_manhattan.jpg\r\n",
      "-rwxrwxrwx 1 angelrps angelrps     202694 Apr  4 12:46 trip_record_user_guide.pdf\r\n"
     ]
    }
   ],
   "source": [
    "!ls -Rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10748623\n",
      "10734230\n",
      "8402723\n"
     ]
    }
   ],
   "source": [
    "# Count Number of Lines\n",
    "!zgrep -c $ 2017_Yellow_Taxi_Trip_Data.csv.bz2\n",
    "!zgrep -c $ 2018_Yellow_Taxi_Trip_Data.csv.bz2\n",
    "!zgrep -c $ 2019_Yellow_Taxi_Trip_Data.csv.bz2\n",
    "\n",
    "# 2017_...10.748.623 lines\n",
    "# 2018_...10.734.230 lines\n",
    "# 2019_...8.402.723 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will start exploring the 2019 dataset\n",
    "I will try to use bash to clean the data as much as I can before importing a Data Frame because I believe is faster.\n",
    "From a previous exploration I concluded:\n",
    "1. There are trips with 0 'trip_distance'. I will remove all trips with 'trip_distance' < 0.06 (100 meters) because I consider them either measurement errors or non representative data.\n",
    "2. There are trips with either negative or very little 'fare_amount' (~0.01). I will remove rows with fare amount<1$ because I consider them measurement errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,congestion_surcharge\r\n",
      "1,04/29/2019 11:31:03 AM,04/29/2019 11:31:03 AM,1,0,1,N,231,264,2,2.5,2.5,0.5,0,0,0.3,5.8,2.5\r\n",
      "2,04/29/2019 10:52:08 AM,04/29/2019 11:06:18 AM,1,1.01,1,N,186,230,1,10,0,0.5,2.66,0,0.3,15.96,2.5\r\n",
      "2,04/29/2019 11:28:40 AM,04/29/2019 11:33:01 AM,1,0.82,1,N,238,151,1,5,0,0.5,1.66,0,0.3,9.96,2.5\r\n",
      "1,04/29/2019 11:28:06 AM,04/29/2019 12:07:32 PM,1,12.1,1,N,138,88,1,38.5,2.5,0.5,10.4,0,0.3,52.2,2.5\r\n",
      "\r\n",
      "bzcat: I/O or other error, bailing out.  Possible reason follows.\r\n",
      "bzcat: Broken pipe\r\n",
      "\tInput file = ./2019_Yellow_Taxi_Trip_Data.csv.bz2, output file = (stdout)\r\n"
     ]
    }
   ],
   "source": [
    "!bzcat ./2019_Yellow_Taxi_Trip_Data.csv.bz2 | head -n 5\n",
    "\n",
    "# Conclusions:\n",
    "# 1. There is a header on the first line\n",
    "# 2. The separator is ','"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean up dataset taking just a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cleaning actions are based on the dataset exploration made in the notebok 'Data_Analysis.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import part of the csv so the memory can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "dfsample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove trips under 0.06 (100m). I consider them either errors or not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([], dtype='int64')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECAP 2\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all trips with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "dfsample[dfsample['trip_distance']<0.06].index # this should be none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove negative 'fare_amount' or <1$. I consider them errors or not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pickup_datetime, VendorID, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECAP 3\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all rows with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "# 3. Remove rows with 'fare_amount' negative or <1$ because I consider them measurement errors or non relevant.\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "dfsample[dfsample['fare_amount']<1] # this should be none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have removed unwanted rows, letÂ´s select the two columns of interest:\n",
    "- 'pickup_datetime'\n",
    "- 'PULocationID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-29 10:52:08</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-29 11:28:40</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-04-29 11:28:06</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-04-29 11:39:32</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2019-04-29 11:06:45</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime  PULocationID\n",
       "1 2019-04-29 10:52:08           186\n",
       "2 2019-04-29 11:28:40           238\n",
       "3 2019-04-29 11:28:06           138\n",
       "4 2019-04-29 11:39:32           138\n",
       "5 2019-04-29 11:06:45           141"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECAP 4\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all rows with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "# 3. Remove rows with 'fare_amount' negative or <1$ because I consider them measurement errors or non relevant.\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "# 4. Select columns of interest.\n",
    "#    'pickup_datetime'\n",
    "#    'PULocationID'\n",
    "dfsample = dfsample[['pickup_datetime','PULocationID']]\n",
    "dfsample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set timestamp as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:52:08</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:28:40</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:28:06</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:39:32</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:06:45</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     PULocationID\n",
       "pickup_datetime                  \n",
       "2019-04-29 10:52:08           186\n",
       "2019-04-29 11:28:40           238\n",
       "2019-04-29 11:28:06           138\n",
       "2019-04-29 11:39:32           138\n",
       "2019-04-29 11:06:45           141"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECAP 5\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all rows with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "# 3. Remove rows with 'fare_amount' negative or <1$ because I consider them measurement errors or non relevant.\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "# 4. Select columns of interest.\n",
    "#    'pickup_datetime'\n",
    "#    'PULocationID'\n",
    "dfsample = dfsample[['pickup_datetime','PULocationID']]\n",
    "\n",
    "# 5. Set timestamp as index\n",
    "dfsample.set_index('pickup_datetime', inplace=True)\n",
    "dfsample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert datetime in periods of 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:00</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 11:00</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PULocationID\n",
       "pickup_datetime               \n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 11:00           238\n",
       "2019-04-29 11:00           138\n",
       "2019-04-29 11:00           138\n",
       "2019-04-29 11:00           141"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECAP 6\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all rows with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "# 3. Remove rows with 'fare_amount' negative or <1$ because I consider them measurement errors or non relevant.\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "# 4. Select columns of interest.\n",
    "#    'pickup_datetime'\n",
    "#    'PULocationID'\n",
    "dfsample = dfsample[['pickup_datetime','PULocationID']]\n",
    "\n",
    "# 5. Set timestamp as index\n",
    "dfsample.set_index('pickup_datetime', inplace=True)\n",
    "\n",
    "# 6. Convert the DATE in PERIODS of 1 hour\n",
    "dfsample = dfsample.to_period(\"H\")\n",
    "dfsample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by datetime and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-04-29 05:00</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 07:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 07:00</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 07:00</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 07:00</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 08:00</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 08:00</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 08:00</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 09:00</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 09:00</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-04-29 10:00</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PULocationID\n",
       "pickup_datetime               \n",
       "2019-04-29 05:00            90\n",
       "2019-04-29 06:00            68\n",
       "2019-04-29 06:00            88\n",
       "2019-04-29 06:00           107\n",
       "2019-04-29 06:00           113\n",
       "2019-04-29 06:00           125\n",
       "2019-04-29 06:00           132\n",
       "2019-04-29 06:00           138\n",
       "2019-04-29 06:00           236\n",
       "2019-04-29 06:00           237\n",
       "2019-04-29 07:00            68\n",
       "2019-04-29 07:00           113\n",
       "2019-04-29 07:00           162\n",
       "2019-04-29 07:00           237\n",
       "2019-04-29 08:00            79\n",
       "2019-04-29 08:00           234\n",
       "2019-04-29 08:00           237\n",
       "2019-04-29 09:00            75\n",
       "2019-04-29 09:00           236\n",
       "2019-04-29 10:00            13\n",
       "2019-04-29 10:00            43\n",
       "2019-04-29 10:00            43\n",
       "2019-04-29 10:00            43\n",
       "2019-04-29 10:00            43\n",
       "2019-04-29 10:00            45\n",
       "2019-04-29 10:00            48\n",
       "2019-04-29 10:00            68\n",
       "2019-04-29 10:00            68\n",
       "2019-04-29 10:00            68\n",
       "2019-04-29 10:00            79\n",
       "2019-04-29 10:00            82\n",
       "2019-04-29 10:00            90\n",
       "2019-04-29 10:00           100\n",
       "2019-04-29 10:00           100\n",
       "2019-04-29 10:00           100\n",
       "2019-04-29 10:00           107\n",
       "2019-04-29 10:00           107\n",
       "2019-04-29 10:00           107\n",
       "2019-04-29 10:00           107\n",
       "2019-04-29 10:00           113\n",
       "2019-04-29 10:00           113\n",
       "2019-04-29 10:00           125\n",
       "2019-04-29 10:00           132\n",
       "2019-04-29 10:00           132\n",
       "2019-04-29 10:00           137\n",
       "2019-04-29 10:00           137\n",
       "2019-04-29 10:00           138\n",
       "2019-04-29 10:00           138\n",
       "2019-04-29 10:00           138\n",
       "2019-04-29 10:00           138\n",
       "2019-04-29 10:00           138\n",
       "2019-04-29 10:00           140\n",
       "2019-04-29 10:00           140\n",
       "2019-04-29 10:00           140\n",
       "2019-04-29 10:00           140\n",
       "2019-04-29 10:00           141\n",
       "2019-04-29 10:00           141\n",
       "2019-04-29 10:00           142\n",
       "2019-04-29 10:00           142\n",
       "2019-04-29 10:00           142\n",
       "2019-04-29 10:00           142\n",
       "2019-04-29 10:00           158\n",
       "2019-04-29 10:00           161\n",
       "2019-04-29 10:00           161\n",
       "2019-04-29 10:00           162\n",
       "2019-04-29 10:00           162\n",
       "2019-04-29 10:00           163\n",
       "2019-04-29 10:00           163\n",
       "2019-04-29 10:00           163\n",
       "2019-04-29 10:00           163\n",
       "2019-04-29 10:00           163\n",
       "2019-04-29 10:00           163\n",
       "2019-04-29 10:00           164\n",
       "2019-04-29 10:00           164\n",
       "2019-04-29 10:00           164\n",
       "2019-04-29 10:00           166\n",
       "2019-04-29 10:00           170\n",
       "2019-04-29 10:00           170\n",
       "2019-04-29 10:00           170\n",
       "2019-04-29 10:00           170\n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 10:00           186\n",
       "2019-04-29 10:00           211\n",
       "2019-04-29 10:00           211\n",
       "2019-04-29 10:00           229\n",
       "2019-04-29 10:00           229\n",
       "2019-04-29 10:00           229\n",
       "2019-04-29 10:00           230\n",
       "2019-04-29 10:00           230\n",
       "2019-04-29 10:00           230\n",
       "2019-04-29 10:00           231\n",
       "2019-04-29 10:00           233\n",
       "2019-04-29 10:00           234\n",
       "2019-04-29 10:00           234\n",
       "2019-04-29 10:00           234"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECAP 7\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all rows with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "# 3. Remove rows with 'fare_amount' negative or <1$ because I consider them measurement errors or non relevant.\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "# 4. Select columns of interest.\n",
    "#    'pickup_datetime'\n",
    "#    'PULocationID'\n",
    "dfsample = dfsample[['pickup_datetime','PULocationID']]\n",
    "\n",
    "# 5. Set timestamp as index\n",
    "dfsample.set_index('pickup_datetime', inplace=True)\n",
    "\n",
    "# 6. Convert the DATE in PERIODS of 1 hour\n",
    "dfsample = dfsample.to_period(\"H\")\n",
    "\n",
    "# 7. Sort by datetime and by Location\n",
    "dfsample.sort_values(['pickup_datetime', 'PULocationID'], inplace=True)\n",
    "dfsample.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new column to count Number of Pickups per zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECAP 8\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all rows with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "# 3. Remove rows with 'fare_amount' negative or <1$ because I consider them measurement errors or non relevant.\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "# 4. Select columns of interest.\n",
    "#    'pickup_datetime'\n",
    "#    'PULocationID'\n",
    "dfsample = dfsample[['pickup_datetime','PULocationID']]\n",
    "\n",
    "# 5. Set timestamp as index\n",
    "dfsample.set_index('pickup_datetime', inplace=True)\n",
    "\n",
    "# 6. Convert the DATE in PERIODS of 1 hour\n",
    "dfsample = dfsample.to_period(\"H\")\n",
    "\n",
    "# 7. Sort by datetime and by Location\n",
    "dfsample.sort_values(['pickup_datetime', 'PULocationID'], inplace=True)\n",
    "\n",
    "# 8. Create new column to count Number of Pickups per zone\n",
    "dfsample['NoOfPickups'] =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby Datetime and Location to get Pickups sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECAP 9\n",
    "\n",
    "# 1. Prepare sample file with 10.000 rows\n",
    "#    Parse 'tpep_pickup_datetime' column to datetime object type\n",
    "#    Import just needed columns to lighten the process\n",
    "dfsample = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Remove all rows with 'trip_distance' < 0.06 (100m) because I consider them measurement errors or non representative values\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "# 3. Remove rows with 'fare_amount' negative or <1$ because I consider them measurement errors or non relevant.\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "# 4. Select columns of interest.\n",
    "#    'pickup_datetime'\n",
    "#    'PULocationID'\n",
    "dfsample = dfsample[['pickup_datetime','PULocationID']]\n",
    "\n",
    "# 5. Set timestamp as index\n",
    "dfsample.set_index('pickup_datetime', inplace=True)\n",
    "\n",
    "# 6. Convert the DATE in PERIODS of 1 hour\n",
    "dfsample = dfsample.to_period(\"H\")\n",
    "\n",
    "# 7. Sort by datetime and by Location\n",
    "dfsample.sort_values(['pickup_datetime', 'PULocationID'], inplace=True)\n",
    "\n",
    "# 8. Create new column to count Number of Pickups per zone\n",
    "dfsample['NoOfPickups'] =1\n",
    "\n",
    "# 9. Groupby Datetime and Location to get Pickups sum()\n",
    "dfsample=dfsample.groupby(['pickup_datetime', 'PULocationID'])['NoOfPickups'].sum().reset_index()\n",
    "dfsample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test Big Data approach on a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO RUN\n",
    "\n",
    "DistIndex_to_drop = dfsample[dfsample['trip_distance'] < 0.06].index\n",
    "dfsample.drop(DistIndex_to_drop, inplace=True)\n",
    "\n",
    "FareIndex_to_drop = dfsample[dfsample['fare_amount']<1].index\n",
    "dfsample.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "dfsample = dfsample[['pickup_datetime','PULocationID']]\n",
    "\n",
    "dfsample.set_index('pickup_datetime', inplace=True)\n",
    "\n",
    "dfsample = dfsample.to_period(\"H\")\n",
    "\n",
    "dfsample.sort_values(['pickup_datetime', 'PULocationID'], inplace=True)\n",
    "\n",
    "dfsample['NoOfPickups'] =1\n",
    "\n",
    "dfsample=dfsample.groupby(['pickup_datetime', 'PULocationID'])['NoOfPickups'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3000\n",
      "1 3000\n",
      "2 3000\n",
      "3 999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>NoOfPickups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-04-29 05:00</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-04-29 06:00</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pickup_datetime  PULocationID  NoOfPickups\n",
       "0  2019-04-29 05:00            90            1\n",
       "1  2019-04-29 06:00            68            1\n",
       "2  2019-04-29 06:00            88            1\n",
       "3  2019-04-29 06:00           107            1\n",
       "4  2019-04-29 06:00           113            1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ITERATION CODE WITH CHUNKS\n",
    "\n",
    "# 1. Get 10000 rows sample iterator with 3000 chunks\n",
    "dfsi = pd.read_csv('./2019_Yellow_Taxi_Trip_Data.csv.bz2', sep=',',\n",
    "                       usecols=['tpep_pickup_datetime','PULocationID','trip_distance','fare_amount'],\n",
    "                       nrows=9999,\n",
    "                       chunksize=3000,\n",
    "                       parse_dates={'pickup_datetime':['tpep_pickup_datetime']})\n",
    "\n",
    "# 2. Declare empty DataFrame to store results of each chunk\n",
    "all_chunks=pd.DataFrame()\n",
    "\n",
    "# 3. Iterate through chunks and append results\n",
    "for i, dfs in enumerate(dfsi):\n",
    "    print(i, len(dfs))    \n",
    "    \n",
    "    # CODE TO RUN\n",
    "    DistIndex_to_drop = dfs[dfs['trip_distance'] < 0.06].index\n",
    "    dfs.drop(DistIndex_to_drop, inplace=True)\n",
    "    \n",
    "    FareIndex_to_drop = dfs[dfs['fare_amount']<1].index\n",
    "    dfs.drop(FareIndex_to_drop, inplace=True)\n",
    "\n",
    "    dfs = dfs[['pickup_datetime','PULocationID']]\n",
    "\n",
    "    dfs.set_index('pickup_datetime', inplace=True)\n",
    "\n",
    "    dfs = dfs.to_period(\"H\")\n",
    "\n",
    "    dfs.sort_values(['pickup_datetime', 'PULocationID'], inplace=True)\n",
    "\n",
    "    dfs['NoOfPickups'] =1\n",
    "    \n",
    "    result=dfs.groupby(['pickup_datetime', 'PULocationID'])['NoOfPickups'].sum().reset_index()\n",
    "    all_chunks=all_chunks.append(result)\n",
    "\n",
    "#groupby puts the grouping column as index so we have to:\n",
    "#reset_index()\n",
    "#groupby again the all_chunks results > sum > sort > reset_index again\n",
    "result_all = all_chunks.sort_values(['pickup_datetime', 'PULocationID']). \\\n",
    "                                    groupby(['pickup_datetime', 'PULocationID'])['NoOfPickups'].sum().reset_index()\n",
    "result_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check that the 'result_all' dataframe and 'dfsample' are the same:\n",
    "- Shape is  in both (279,3)\n",
    "- Head and Tail rows are the same\n",
    "- Taking a random row (e.g. row 52), they are both the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pickup_datetime  PULocationID  NoOfPickups\n",
      "274  2019-04-29 13:00           229            1\n",
      "275  2019-04-29 13:00           230            1\n",
      "276  2019-04-29 13:00           231            1\n",
      "277  2019-04-29 13:00           234            1\n",
      "278  2019-04-29 13:00           236            2\n",
      "      pickup_datetime  PULocationID  NoOfPickups\n",
      "274  2019-04-29 13:00           229            1\n",
      "275  2019-04-29 13:00           230            1\n",
      "276  2019-04-29 13:00           231            1\n",
      "277  2019-04-29 13:00           234            1\n",
      "278  2019-04-29 13:00           236            2\n"
     ]
    }
   ],
   "source": [
    "print(dfsample.tail())\n",
    "print(result_all.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pickup_datetime  PULocationID  NoOfPickups\n",
      "0  2019-04-29 05:00            90            1\n",
      "1  2019-04-29 06:00            68            1\n",
      "2  2019-04-29 06:00            88            1\n",
      "3  2019-04-29 06:00           107            1\n",
      "4  2019-04-29 06:00           113            1\n",
      "    pickup_datetime  PULocationID  NoOfPickups\n",
      "0  2019-04-29 05:00            90            1\n",
      "1  2019-04-29 06:00            68            1\n",
      "2  2019-04-29 06:00            88            1\n",
      "3  2019-04-29 06:00           107            1\n",
      "4  2019-04-29 06:00           113            1\n"
     ]
    }
   ],
   "source": [
    "print(dfsample.head())\n",
    "print(result_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_datetime    2019-04-29 10:00\n",
      "PULocationID                    237\n",
      "NoOfPickups                       4\n",
      "Name: 52, dtype: object\n",
      "pickup_datetime    2019-04-29 10:00\n",
      "PULocationID                    237\n",
      "NoOfPickups                       4\n",
      "Name: 52, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dfsample.loc[52])\n",
    "print(result_all.loc[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERSONAL NOTES:\n",
    "# sort by datetime after compiling unique csv\n",
    "# interpolate missing values every hour"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
