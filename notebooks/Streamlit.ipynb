{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit\n",
    "\n",
    "In this notebook contains the final code to create the Streamlit app file:\n",
    "\n",
    "``streamlit_app.py``\n",
    "\n",
    "This application scrapes weather precipitation data from ```www.wunderground.com``` prior to calculate and show predictions on the screen. The web scraping takes about 20 seconds but it just needs to be done once.  \n",
    "\n",
    "This 'waiting time' is the reason I have divided the work into two notebooks:\n",
    "1. **```Streamlit.ipynb```**: contains code and creates script file ```streamlit_app.py```.\n",
    "2. **```Streamlit_JustRun.ipynb```**: it just run the ```.py```script to build the local server.\n",
    "\n",
    "This way I can keep ```Streamlit_JustRun.ipynb``` running (with the server open) while I do tests or edit the script ```.py``` file and see the changes in real time, without having to stop/run the server (and scrape data) every time I change the script.\n",
    "\n",
    "\n",
    "### 0. [Requirements](#Requirements)\n",
    "- [About Streamlit and Bokeh versions](#About-Streamlit-and-Bokeh-versions)\n",
    "- [Chromedriver.exe](#Chromedriver.exe)\n",
    "\n",
    "### 1. [Helper functions](#Helper-functions)\n",
    "- ``get_LocationIDs()``\n",
    "- ``datetimeInfo_and_LocID()``\n",
    "- ``scrape_data()``\n",
    "- ``get_input_data()``\n",
    "- ``get_output_data()``\n",
    "- ``load_shape_data()``\n",
    "- ``load_taxis_data()``\n",
    "\n",
    "### 2. [Final script](#Final-script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Streamlit and Bokeh versions\n",
    "\n",
    "Streamlit before 0.57 only works with Bokeh 1.0 and Streamlit 0.57+ only works with Bokeh 2.<br>\n",
    "I have used **Streamlit 0.62.0** and **Bokeh 2.1.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Streamlit and Bokeh if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit\n",
    "#!pip install bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which version is in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit --version\n",
    "!bokeh info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromedriver.exe\n",
    "\n",
    "In order to scrape weather data from ``wunderground.com`` I had to use ``WebDriver`` with ``Chromedriver.exe``.  \n",
    "\n",
    "**Why?**  \n",
    "\n",
    "``wunderground.com`` seems to have some security feature which blocks known spider/bot user agents (like ``urllib`` used by python).\n",
    "\n",
    "I didnÂ´t want to pay for their API, so I simulate that I am accessing from a known browser user agent (i.e. Chrome).\n",
    "\n",
    "This is why I use **Selenium WebDriver**. ``webdriver`` drives a browser natively, as a user would.\n",
    "\n",
    "Make sure that you have downloaded [Chromedriver.exe](./chromedriver.exe) and that the relative path to where the server has been started is correct.  \n",
    "I run the Jupyter server inside the ``./notebooks`` folder so I save ``chromedriver.exe`` in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Helper functions\n",
    "<div style = \"float:right\"><a style=\"text-decoration:none\" href = \"#Streamlit\">Up</a></div>\n",
    "\n",
    "Explanation about the functions used in the script:\n",
    "\n",
    "**``get_LocationIDs()``**:  \n",
    "Creates a DataFrame with the LocationID of Manhattan zones\n",
    "\n",
    "\n",
    "**``datetimeInfo_and_LocID(df_LocIds, start_date, NoOfDays)``**:  \n",
    "Creates a DataFrame with the Datetime info and LocationID and make the appropriate transforms to pass it on to the predictive model.\n",
    "It uses tomorrows day, up to 3 more days.\n",
    "\n",
    "\n",
    "**``scrape_data(today, days_in)``**:  \n",
    "Scrape Precipitation forecast from *wunderground.com*.  \n",
    "From tomorrow up to 3 more days.\n",
    "\n",
    "\n",
    "**``get_input_data(start_date, NoOfDays)``**:  \n",
    "It takes the outputs from ``datetimeInfo_and_LocID`` and ``scrape_data`` and creates another DataFrame, with the right shape and ready to be taken by the predictive model.\n",
    "\n",
    "\n",
    "**``get_output_data(pickle_file, input_data)``**:  \n",
    "It passes the output of ``get_input_data()`` on to the predictive model and outputs a result DataFrame with: ``dayofweek``, ``hour``, ``LocationID`` and ``pickups``.\n",
    "\n",
    "\n",
    "**``load_shape_data()``**:  \n",
    "Creates a DataFrame with LocationIDs and their associated (X,Y) coordinates so that they can be plotted as a map.\n",
    "\n",
    "\n",
    "**``load_taxis_data(output_data, shape_data)``**:  \n",
    "It takes outputs from ``get_output_data`` and ``load_shape_data`` and transforms the tables so that it can be plotted in the map. It associates slider values to different columns.\n",
    "\n",
    "**``select_day(gdf_merged, weekday1, weekday2, weekday3, selected_weekday)``**:\n",
    "It takes the data returned by ``load_taxis_data()``, filters by selected day and transforms it to wide format so that is can be plot in the line chart.\n",
    "\n",
    "**``create_map_plot(df_to_visualize, _weekday , _hour)``**:\n",
    "It creates a choropleth map showing Manhattan pickups in a specific time frame. It changes when different times and dates are selected.\n",
    "\n",
    "**``create_altair_plots(long_df)``**:\n",
    "It creates a line chart of Manhattan pickups evolution over a day. It changes when a different day is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bokeh.io import output_notebook, output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool, Select, ColumnDataSource, WheelZoomTool, LogColorMapper, LinearColorMapper, ColorBar, BasicTicker\n",
    "from bokeh.palettes import Viridis256 as palette\n",
    "from bokeh.layouts import row\n",
    "import altair as alt\n",
    "\n",
    "# GET LOCATION ID DATA FRAME\n",
    "# when deploying to external server, consider create LocationIDs manually instead of reading csv\n",
    "@st.cache(show_spinner=False)\n",
    "def get_LocationIDs():\n",
    "    # 1. Import Location and Borough columns form NY TAXI ZONES dataset\n",
    "    dfzones = pd.read_csv('https://raw.github.com/angelrps/MasterDataScience_FinalProject/master/data/NY_taxi_zones.csv', sep=',',\n",
    "                          usecols=['LocationID', 'borough'])\n",
    "\n",
    "    # 2. Filter Manhattan zones\n",
    "    dfzones = dfzones[dfzones['borough']=='Manhattan']\\\n",
    "                    .drop(['borough'], axis=1)\\\n",
    "                    .sort_values(by='LocationID')\\\n",
    "                    .drop_duplicates('LocationID').reset_index(drop=True)    \n",
    "    return dfzones\n",
    "\n",
    "# CREATE DATETIME INFO AND APPEND LOCATION IDs\n",
    "@st.cache(show_spinner=False)\n",
    "def datetimeInfo_and_LocID(df_LocIds, start_date, NoOfDays):   \n",
    "\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    \n",
    "    # repeat LocationIDs. All of them... for each hour\n",
    "    location_id_col = pd.concat([df_LocIds]*24*NoOfDays).reset_index(drop=True)\n",
    "\n",
    "    # create data frame with range of days with hourly period\n",
    "    df_pred = pd.DataFrame()\n",
    "    dates = pd.date_range(start = start_date, end = start_date + timedelta(days=NoOfDays), freq = \"H\")\n",
    "    df_pred['datetime'] = dates\n",
    "    df_pred.drop([df_pred.shape[0]-1], inplace=True)\n",
    "\n",
    "    # Create new columns from datetime\n",
    "    df_pred['month'] = df_pred['datetime'].dt.month\n",
    "    df_pred['hour'] = df_pred['datetime'].dt.hour\n",
    "    # 'dayhour' will serve as index to perform the join\n",
    "    df_pred['dayhour'] = df_pred['datetime'].dt.strftime('%d%H')\n",
    "    df_pred['week'] = df_pred['datetime'].dt.week\n",
    "    df_pred['dayofweek'] = df_pred['datetime'].dt.dayofweek\n",
    "\n",
    "\n",
    "    # Create date time index calendar\n",
    "    drange = pd.date_range(start=str(start_date.year)+'-01-01', end=str(start_date.year)+'-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=drange.min(), end=drange.max())\n",
    "    \n",
    "    # 8.3 create new columns 'date' and 'isholiday'\n",
    "    df_pred['date'] = pd.to_datetime(df_pred['datetime'].dt.date)\n",
    "    df_pred['isholiday'] = df_pred['datetime'].isin(holidays).astype(int)\n",
    "    \n",
    "    # drop 'date' and 'datetime' column\n",
    "    df_pred.drop(['datetime'], axis=1, inplace=True)\n",
    "    df_pred.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "    # repeat rows. 67 rows per hour\n",
    "    df_pred = df_pred.iloc[np.arange(len(df_pred)).repeat(len(df_LocIds))].reset_index(drop=True)\n",
    "    #df_index = df_index.iloc[np.arange(len(df_index)).repeat(67)].reset_index(drop=True)\n",
    "\n",
    "    df_pred = df_pred.join(location_id_col)\n",
    "    \n",
    "    return df_pred\n",
    "\n",
    "# SCRAPE PRECIPITATION FORECAST FROM wunderground.com\n",
    "@st.cache(show_spinner=False)\n",
    "def scrape_data(today, days_in):\n",
    "    with st.spinner(\"I am scraping weather data from wunderground.com... please wait.\"):\n",
    "        # Use .format(YYYY, M, D)\n",
    "        lookup_URL = 'https://www.wunderground.com/hourly/us/ny/new-york-city/date/{}-{}-{}.html'\n",
    "\n",
    "        options = webdriver.ChromeOptions();\n",
    "        options.add_argument('headless'); # to run chrome in the backbroung\n",
    "\n",
    "        driver = webdriver.Chrome(executable_path='./chromedriver.exe', options=options)\n",
    "\n",
    "        start_date = today + pd.Timedelta(days=1)\n",
    "        end_date = today + pd.Timedelta(days=days_in + 1)\n",
    "\n",
    "        df_prep = pd.DataFrame()\n",
    "\n",
    "        while start_date != end_date:\n",
    "            timestamp = pd.Timestamp(str(start_date)+' 00:00:00')\n",
    "\n",
    "            print('gathering data from: ', start_date)\n",
    "\n",
    "            formatted_lookup_URL = lookup_URL.format(start_date.year,\n",
    "                                                     start_date.month,\n",
    "                                                     start_date.day)\n",
    "\n",
    "            driver.get(formatted_lookup_URL)\n",
    "            rows = WebDriverWait(driver, 60).until(EC.visibility_of_all_elements_located((By.XPATH, '//td[@class=\"mat-cell cdk-cell cdk-column-liquidPrecipitation mat-column-liquidPrecipitation ng-star-inserted\"]')))\n",
    "            for row in rows:\n",
    "                hour = timestamp.strftime('%H')\n",
    "                day = timestamp.strftime('%d')\n",
    "                prep = row.find_element_by_xpath('.//span[@class=\"wu-value wu-value-to\"]').text\n",
    "                # append new row to table\n",
    "                # 'dayhour' column will serve as column index to perform the Join\n",
    "                df_prep = df_prep.append(pd.DataFrame({\"dayhour\":[day+hour], 'precipitation':[prep]}),\n",
    "                                         ignore_index = True)\n",
    "\n",
    "                timestamp += pd.Timedelta('1 hour')\n",
    "\n",
    "            start_date += timedelta(days=1)\n",
    "    return df_prep\n",
    "\n",
    "# GET INPUT DATA USING THE FUNCTIONS ABOVE: LocationIDs and Datetime info\n",
    "@st.cache(show_spinner=False)\n",
    "def get_input_data(start_date, NoOfDays):\n",
    "    # get LocationIDs data frame\n",
    "    df_LocIds = get_LocationIDs()\n",
    "\n",
    "    # create datetime info and append LocationsIDs\n",
    "    dtInfo_and_LocID = datetimeInfo_and_LocID(df_LocIds,start_date,NoOfDays)\n",
    "\n",
    "    # get precipitation forecast\n",
    "    prep_forecast = scrape_data(date.today(), NoOfDays)\n",
    "\n",
    "    # merge both data frames\n",
    "    df_merged = dtInfo_and_LocID.merge(prep_forecast, on=\"dayhour\", how=\"left\")\n",
    "\n",
    "    # drop dayhour column\n",
    "    df_merged = df_merged.drop(['dayhour'], axis=1)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# GET OUPPUT DATA: get predictions, append to input_data and format it to be processed\n",
    "@st.cache(show_spinner=False)\n",
    "def get_output_data(pickle_file, input_data):\n",
    "    with st.spinner(\"Making predictions...\"):\n",
    "        import pickle\n",
    "\n",
    "        model = pickle.load(open(pickle_file,'rb'))\n",
    "\n",
    "        # get prediction, convert to integer and convert Array into DataFrame\n",
    "        model_predict = (model.predict(input_data)).astype(int)\n",
    "        df_predict = pd.DataFrame({'pickups':model_predict})\n",
    "\n",
    "        # join input_data with DataFrame\n",
    "        joined = input_data.join(df_predict)\n",
    "\n",
    "        output_data = joined[['hour','dayofweek','LocationID','pickups']]\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "# GET DATA FRAME WITH SHAPE GEOMETRY INFO\n",
    "@st.cache(show_spinner=False)\n",
    "def load_shape_data():\n",
    "    path = '../data/taxi_zones/taxi_zones.shp'\n",
    "    shape_data = gpd.read_file(path)\n",
    "\n",
    "    # filter Manhattan zones\n",
    "    shape_data = shape_data[shape_data['borough'] == 'Manhattan'].reset_index(drop=True)\n",
    "\n",
    "    shape_data = shape_data.drop(['borough'], axis=1)\n",
    "\n",
    "    #EPSG-Code of Web Mercador\n",
    "    shape_data.to_crs(epsg=3785, inplace=True)\n",
    "\n",
    "    # Simplify Shape of Zones (otherwise slow peformance of plot)\n",
    "    shape_data[\"geometry\"] = shape_data[\"geometry\"].simplify(100)\n",
    "\n",
    "    data = []\n",
    "    for zonename, LocationID, shape in shape_data[[\"zone\", \"LocationID\", \"geometry\"]].values:\n",
    "        #If shape is polygon, extract X and Y coordinates of boundary line:\n",
    "        if isinstance(shape, Polygon):\n",
    "            X, Y = shape.boundary.xy\n",
    "            X = [int(x) for x in X]\n",
    "            Y = [int(y) for y in Y]\n",
    "            data.append([LocationID, zonename, X, Y])\n",
    "\n",
    "        #If shape is Multipolygon, extract X and Y coordinates of each sub-Polygon:\n",
    "        if isinstance(shape, MultiPolygon):\n",
    "            for poly in shape:\n",
    "                X, Y = poly.boundary.xy\n",
    "                X = [int(x) for x in X]\n",
    "                Y = [int(y) for y in Y]\n",
    "                data.append([LocationID, zonename, X, Y])\n",
    "\n",
    "    #Create new DataFrame with X an Y coordinates separated:\n",
    "    shape_data = pd.DataFrame(data, columns=[\"LocationID\", \"ZoneName\", \"X\", \"Y\"])\n",
    "    return shape_data\n",
    "\n",
    "@st.cache(allow_output_mutation=True, show_spinner=False)\n",
    "def load_taxis_data(output_data, shape_data):\n",
    "    df_to_visualize = shape_data.copy()\n",
    "    pickups = output_data.groupby(['hour','dayofweek','LocationID']).sum()\n",
    "    #start_day = pd.unique(output_data['dayofweek']).min()\n",
    "    #end_day = pd.unique(output_data['dayofweek']).max()\n",
    "    listofdays = pd.unique(output_data['dayofweek'])\n",
    "\n",
    "    for hour in range(24):\n",
    "        #for dayofweek in range(start_day,end_day+1,1):\n",
    "        for dayofweek in listofdays:\n",
    "            # get pickups for this hour and weekday\n",
    "            p = pd.DataFrame(pickups.loc[(hour, dayofweek)]).reset_index()\n",
    "        \n",
    "            # add pickups to the Taxi Zones DataFrame       \n",
    "            df_to_visualize = pd.merge(df_to_visualize, p, on=\"LocationID\", how=\"left\").fillna(0)\n",
    "            # rename column as per day and hour\n",
    "            df_to_visualize.rename(columns={\"pickups\" : \"Passenger_%d_%d\"%(dayofweek, hour)}, inplace=True)\n",
    "\n",
    "    return df_to_visualize\n",
    "\n",
    "@st.cache(show_spinner=False, allow_output_mutation=True)\n",
    "def select_day(gdf_merged, weekday1, weekday2, weekday3, selected_weekday):\n",
    "    gdf_merged_c = gdf_merged.copy()\n",
    "    day_list = [weekday1, weekday2, weekday3]\n",
    "    day_list.remove(selected_weekday)\n",
    "    for hour in range(24):\n",
    "        for dayofweek in day_list:\n",
    "            column_to_drop = \"Passenger_%d_%d\"%(dayofweek, hour)\n",
    "            gdf_merged_c.drop([column_to_drop], axis=1, inplace=True)\n",
    "    gdf_merged_c.reset_index(level=0, inplace=True)\n",
    "    return gdf_merged_c\n",
    "\n",
    "def create_altair_plots(long_df):\n",
    "    # create selections\n",
    "    selection = alt.selection_multi(fields=['ZoneName'])\n",
    "    sel_size = alt.selection_single(empty='none')\n",
    "    sel_size_legend = alt.selection_multi(fields=['ZoneName'], empty='none')\n",
    "    sel_line_hover = alt.selection_single(on='mouseover', empty='none')\n",
    "\n",
    "    opacity = alt.condition(selection, alt.value(1), alt.value(0.5))\n",
    "\n",
    "    color = alt.condition(selection,\n",
    "                         alt.Color('ZoneName:O', legend=None, scale=alt.Scale(scheme='category10')),\n",
    "                         alt.value('lightgray'))\n",
    "\n",
    "    line = alt.Chart(long_df).mark_line().encode(\n",
    "        x='hour',\n",
    "        y=alt.Y('Passenger_{0}_'.format(weekday), title='pickups'),\n",
    "        color=alt.Color('ZoneName', legend=None),\n",
    "        size=alt.condition(sel_line_hover|sel_size|sel_size_legend, alt.value(4),alt.value(1)),\n",
    "        opacity = opacity,\n",
    "        tooltip = alt.Tooltip('ZoneName:O')\n",
    "        ).properties(\n",
    "        width=550,\n",
    "        height=750,\n",
    "        ).add_selection(\n",
    "        selection, sel_line_hover,sel_size,sel_size_legend\n",
    "        )\n",
    "\n",
    "    legend = alt.Chart(long_df).mark_point(\n",
    "        filled=True, size=50\n",
    "        ).encode(\n",
    "        y=alt.Y('ZoneName', axis=alt.Axis(orient='right'), title=None),\n",
    "        color=color,\n",
    "        ).properties(\n",
    "        width=20,\n",
    "        height=750,\n",
    "        ).add_selection(\n",
    "        selection,sel_size_legend\n",
    "        )\n",
    "    \n",
    "    return line, legend\n",
    "\n",
    "def create_map_plot(df_to_visualize, _weekday , _hour):\n",
    "    df_for_map = df_to_visualize.copy()\n",
    "    # ColumnDataSource transforms the data into something that Bokeh and Java understand\n",
    "    df_for_map[\"Passengers\"] = df_for_map[\"Passenger_\" + str(_weekday) + \"_\" + str(_hour)]\n",
    "\n",
    "    source = ColumnDataSource(df_for_map)\n",
    "\n",
    "    max_passengers_per_hour = df_for_map[filter(lambda x: \"Passenger_\" in x, df_for_map.columns)].max().max()\n",
    "\n",
    "    color_mapper = LinearColorMapper(palette=palette[::-1], high=max_passengers_per_hour, low=0)\n",
    "\n",
    "    ##### Color Bar\n",
    "    color_bar = ColorBar(color_mapper = color_mapper,\n",
    "                         ticker = BasicTicker(),\n",
    "                        label_standoff=8,\n",
    "                         location=(0,0),\n",
    "                         orientation='vertical')\n",
    "\n",
    "    p = figure(plot_width=450, plot_height=750,\n",
    "               toolbar_location=None,\n",
    "               tools='pan,wheel_zoom,box_zoom,reset,save')\n",
    "    p.xaxis.visible = False\n",
    "    p.yaxis.visible = False\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    # Get rid of zoom on axes:\n",
    "    for t in p.tools:\n",
    "        if type(t) == WheelZoomTool:\n",
    "            t.zoom_on_axis = False\n",
    "\n",
    "    patches = p.patches(xs=\"X\", ys=\"Y\", source=source,fill_alpha=1,\n",
    "                      fill_color={'field': 'Passengers',\n",
    "                                  'transform': color_mapper},\n",
    "                      line_color=\"black\", alpha=0.5)\n",
    "\n",
    "    hovertool = HoverTool(tooltips=[('Zone:', \"@ZoneName\"),\n",
    "                                    (\"Passengers:\", \"@Passengers\")])\n",
    "    p.add_tools(hovertool)\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    return p\n",
    "#############################   DEFINE FUNCTIONS END   #############################\n",
    "\n",
    "# INITIAL SET PAGE CONFIG\n",
    "page_title = 'Taxi Demand Predictor'\n",
    "layout='wide'\n",
    "initial_sidebar_state = 'expanded'\n",
    "\n",
    "# SHOW TITLE AND DESCRIPTION\n",
    "st.title(\"Manhattan Taxi Demand Predictor\")\n",
    "\n",
    "st.markdown(\"This is my Final Master's work (Master in Data Science - [KSchool](https://www.kschool.com/)).\\\n",
    "            This Machine Learning app allows you to predict taxi pickups demand in Manhattan for the next 3 days!\\\n",
    "            Choose your options from the side bar.\")\n",
    "\n",
    "# DECLARE VARIABLES: start date, NoOfDays, pickle_file\n",
    "start_date = date.today() + timedelta(days=1) # start day is tomorrow\n",
    "NoOfDays = 3 # number of days for prediction\n",
    "pickle_file = './model_regGB.pickle'\n",
    "\n",
    "# SCRAPE WEATHER, PREPARE DATA, MAKE PREDICTIONS\n",
    "input_data = get_input_data(start_date, NoOfDays)\n",
    "\n",
    "output_data = get_output_data(pickle_file, input_data)\n",
    "\n",
    "shape_data = load_shape_data()\n",
    "\n",
    "df_to_visualize = load_taxis_data(output_data,shape_data)\n",
    "\n",
    "\n",
    "# SIDE BAR - TITLE\n",
    "st.sidebar.title('Your options:')\n",
    "\n",
    "choose_graph = st.sidebar.selectbox(\n",
    "    'Choose your prefered visualization:',\n",
    "     ['Map', 'Line Chart'])\n",
    "\n",
    "# SIDE BAR - SLIDER: DAYS\n",
    "day1 = date.today() + pd.Timedelta(days=1)\n",
    "day2 = date.today() + pd.Timedelta(days=2)\n",
    "day3 = date.today() + pd.Timedelta(days=3)\n",
    "\n",
    "choosen_day = st.sidebar.selectbox('Day to look at:', [day1, day2, day3])\n",
    "selected_day = str(choosen_day)\n",
    "weekday = choosen_day.weekday()\n",
    "\n",
    "# SIDE BAR - SLIDER: HOURS\n",
    "hour=7\n",
    "hour = st.sidebar.slider(\"Hour to look at:\",min_value=0, max_value=23, value=7, step=1)\n",
    "\n",
    "# PREPARE DATA FOR LINE CHART    \n",
    "# filter selected day data to show on map\n",
    "gdf_selected_day = select_day(df_to_visualize,\n",
    "                              day1.weekday(),\n",
    "                              day2.weekday(),\n",
    "                              day3.weekday(),\n",
    "                              weekday)\n",
    "\n",
    "# transform format from wide to long    \n",
    "long_df = pd.wide_to_long(gdf_selected_day, [\"Passenger_{0}_\".format(weekday)], i='index', j=\"hour\")\n",
    "long_df = long_df.reset_index()\n",
    "\n",
    "# filter wrong negative predictions\n",
    "long_df.loc[long_df[\"Passenger_{0}_\".format(weekday)]<0, 'Passenger_{0}_'.format(weekday)] = 0\n",
    "\n",
    "# CREATE PLOTS\n",
    "line, legend = create_altair_plots(long_df)\n",
    "#map_plot = create_map_plot(df_to_visualize)\n",
    "\n",
    "if choose_graph == 'Map':\n",
    "    st.markdown(\"*\" + \"Pickups between %i:00 and %i:00: \" % (hour, (hour + 1) % 24) + selected_day + \"*\")\n",
    "    st.bokeh_chart(create_map_plot(df_to_visualize,weekday,hour))\n",
    "if choose_graph == 'Line Chart':\n",
    "    st.markdown(\"*Evolution over the day: \" + selected_day+\"*\")\n",
    "    st.altair_chart(line | legend)   \n",
    "\n",
    "# SIDE BAR - TOP 3 TABLES\n",
    "column_name = \"Passenger_{0}_\".format(weekday)\n",
    "top5_hour = long_df.copy()\n",
    "top5_hour = top5_hour[top5_hour['hour']==hour]\n",
    "top5_hour = top5_hour.groupby(['ZoneName']).mean().sort_values(column_name, ascending=False)\n",
    "top5_hour.rename(columns={column_name: 'Passengers'}, inplace=True)\n",
    "top5_hour['Passengers'] = top5_hour['Passengers'].astype(int)\n",
    "st.sidebar.subheader('Top 3 areas per HOUR:')\n",
    "st.sidebar.table(top5_hour[ 'Passengers'].head(3))\n",
    "\n",
    "top5_day = long_df.copy()\n",
    "top5_day = top5_day.groupby('ZoneName').mean().sort_values(column_name, ascending=False)\n",
    "top5_day.rename(columns={column_name: 'Passengers'}, inplace=True)\n",
    "top5_day['Passengers'] = top5_day['Passengers'].astype(int)\n",
    "st.sidebar.subheader('Top 3 areas per DAY:')\n",
    "st.sidebar.table(top5_day[ 'Passengers'].head(3))\n",
    "\n",
    "# ABOUT ME\n",
    "st.markdown('*About me: [angelrps.com](https://www.angelrps.com/) | [linkedin](https://www.linkedin.com/in/angelruizpeinado/)*')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
