{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit\n",
    "\n",
    "In this notebook contains the final code to create the Streamlit app file:\n",
    "\n",
    "``streamlit_app.py``\n",
    "\n",
    "### 0. [Requirements](#Requirements)\n",
    "- [About Streamlit and Bokeh versions](#About-Streamlit-and-Bokeh-versions)\n",
    "- [Chromedriver.exe](#Chromedriver.exe)\n",
    "\n",
    "### 1. [Helper functions](#Helper-functions)\n",
    "- ``get_LocationIDs()``\n",
    "- ``datetimeInfo_and_LocID()``\n",
    "- ``scrape_data()``\n",
    "- ``get_input_data()``\n",
    "- ``get_output_data()``\n",
    "- ``load_shape_data()``\n",
    "- ``load_taxis_data()``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Streamlit and Bokeh versions\n",
    "\n",
    "Streamlit before 0.57 only works with Bokeh 1.0 and Streamlit 0.57+ only works with Bokeh 2.<br>\n",
    "I have used **Streamlit 0.62.0** and **Bokeh 2.1.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Streamlit and Bokeh if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit\n",
    "#!pip install bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which version is in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit --version\n",
    "!bokeh info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromedriver.exe\n",
    "\n",
    "In order to scrape weather data from ``wunderground.com`` I had to use ``WebDriver`` with ``Chromedriver.exe``.  \n",
    "\n",
    "**Why?**  \n",
    "\n",
    "``wunderground.com`` seems to have some security feature which blocks known spider/bot user agents (like ``urllib`` used by python).\n",
    "\n",
    "I didnÂ´t want to pay for their API, so I simulate that I am accessing from a known browser user agent (i.e. Chrome).\n",
    "\n",
    "This is why I use **Selenium WebDriver**. ``webdriver`` drives a browser natively, as a user would.\n",
    "\n",
    "Make sure that you have downloaded [Chromedriver.exe](./chromedriver.exe) and that the relative path to where the server has been started is correct.  \n",
    "I run the Jupyter server inside the ``./notebooks`` folder so I save ``chromedriver.exe`` in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Helper functions\n",
    "<div style = \"float:right\"><a style=\"text-decoration:none\" href = \"#Streamlit\">Up</a></div>\n",
    "\n",
    "Explanation about the functions used in the script:\n",
    "\n",
    "**``get_LocationIDs()``**:  \n",
    "Creates a DataFrame with the LocationID of Manhattan zones\n",
    "\n",
    "\n",
    "**``datetimeInfo_and_LocID(df_LocIds, start_date, NoOfDays)``**:  \n",
    "Creates a DataFrame with the Datetime info and LocationID and make the appropriate transforms to pass it on to the predictive model.\n",
    "It uses tomorrows day, up to 3 more days.\n",
    "\n",
    "\n",
    "**``scrape_data(today, days_in)``**:  \n",
    "Scrape Precipitation forecast from *wunderground.com*.  \n",
    "From tomorrow up to 3 more days.\n",
    "\n",
    "\n",
    "**``get_input_data(start_date, NoOfDays)``**:  \n",
    "It takes the outputs from ``datetimeInfo_and_LocID`` and ``scrape_data`` and creates another DataFrame, with the right shape and ready to be taken by the predictive model.\n",
    "\n",
    "\n",
    "**``get_output_data(pickle_file, input_data)``**:  \n",
    "It passes the output of ``get_input_data()`` on to the predictive model and outputs a result DataFrame with: ``dayofweek``, ``hour``, ``LocationID`` and ``pickups``.\n",
    "\n",
    "\n",
    "**``load_shape_data()``**:  \n",
    "Creates a DataFrame with LocationIDs and their associated (X,Y) coordinates so that they can be plotted as a map.\n",
    "\n",
    "\n",
    "**``load_taxis_data(output_data, shape_data)``**:  \n",
    "It takes outputs from ``get_output_data`` and ``load_shape_data`` and transforms the tables so that it can be plotted. It associates slider values to different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bokeh.io import output_notebook, output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool, Select, ColumnDataSource, WheelZoomTool, LogColorMapper, LinearColorMapper, ColorBar, BasicTicker\n",
    "from bokeh.palettes import Viridis256 as palette\n",
    "from bokeh.layouts import row\n",
    "\n",
    "\n",
    "#############################   DEFINE FUNCTIONS START   #############################\n",
    "\n",
    "# GET LOCATION ID DATA FRAME\n",
    "# when deploying to external server, consider create LocationIDs manually instead of reading csv\n",
    "@st.cache\n",
    "def get_LocationIDs():\n",
    "    # 1. Import Location and Borough columns form NY TAXI ZONES dataset\n",
    "    dfzones = pd.read_csv('../data/NY_taxi_zones.csv', sep=',',\n",
    "                          usecols=['LocationID', 'borough'])\n",
    "\n",
    "    # 2. Filter Manhattan zones\n",
    "    dfzones = dfzones[dfzones['borough']=='Manhattan']\\\n",
    "                    .drop(['borough'], axis=1)\\\n",
    "                    .sort_values(by='LocationID')\\\n",
    "                    .drop_duplicates('LocationID').reset_index(drop=True)    \n",
    "    return dfzones\n",
    "\n",
    "# CREATE DATETIME INFO AND APPEND LOCATION IDs\n",
    "@st.cache\n",
    "def datetimeInfo_and_LocID(df_LocIds, start_date, NoOfDays):   \n",
    "\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    \n",
    "    # repeat LocationIDs. All of them... for each hour\n",
    "    location_id_col = pd.concat([df_LocIds]*24*NoOfDays).reset_index(drop=True)\n",
    "\n",
    "    # create data frame with range of days with hourly period\n",
    "    df_pred = pd.DataFrame()\n",
    "    dates = pd.date_range(start = start_date, end = start_date + timedelta(days=NoOfDays), freq = \"H\")\n",
    "    df_pred['datetime'] = dates\n",
    "    df_pred.drop([df_pred.shape[0]-1], inplace=True)\n",
    "\n",
    "    # Create new columns from datetime\n",
    "    df_pred['month'] = df_pred['datetime'].dt.month\n",
    "    df_pred['hour'] = df_pred['datetime'].dt.hour\n",
    "    # 'dayhour' will serve as index to perform the join\n",
    "    df_pred['dayhour'] = df_pred['datetime'].dt.strftime('%d%H')\n",
    "    df_pred['week'] = df_pred['datetime'].dt.week\n",
    "    df_pred['dayofweek'] = df_pred['datetime'].dt.dayofweek\n",
    "\n",
    "\n",
    "    # Create date time index calendar\n",
    "    drange = pd.date_range(start=str(start_date.year)+'-01-01', end=str(start_date.year)+'-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=drange.min(), end=drange.max())\n",
    "    \n",
    "    # 8.3 create new columns 'date' and 'isholiday'\n",
    "    df_pred['date'] = pd.to_datetime(df_pred['datetime'].dt.date)\n",
    "    df_pred['isholiday'] = df_pred['datetime'].isin(holidays).astype(int)\n",
    "    \n",
    "    # drop 'date' and 'datetime' column\n",
    "    df_pred.drop(['datetime'], axis=1, inplace=True)\n",
    "    df_pred.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "    # repeat rows. 67 rows per hour\n",
    "    df_pred = df_pred.iloc[np.arange(len(df_pred)).repeat(len(df_LocIds))].reset_index(drop=True)\n",
    "    #df_index = df_index.iloc[np.arange(len(df_index)).repeat(67)].reset_index(drop=True)\n",
    "\n",
    "    df_pred = df_pred.join(location_id_col)\n",
    "    \n",
    "    return df_pred\n",
    "\n",
    "# SCRAPE PRECIPITATION FORECAST FROM wunderground.com\n",
    "@st.cache\n",
    "def scrape_data(today, days_in):\n",
    "    # Use .format(YYYY, M, D)\n",
    "    lookup_URL = 'https://www.wunderground.com/hourly/us/ny/new-york-city/date/{}-{}-{}.html'\n",
    "\n",
    "    options = webdriver.ChromeOptions();\n",
    "    options.add_argument('headless'); # to run chrome in the backbroung\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver.exe', options=options)\n",
    "\n",
    "    start_date = today + pd.Timedelta(days=1)\n",
    "    end_date = today + pd.Timedelta(days=days_in + 1)\n",
    "\n",
    "    df_prep = pd.DataFrame()\n",
    "\n",
    "    while start_date != end_date:\n",
    "        timestamp = pd.Timestamp(str(start_date)+' 00:00:00')\n",
    "        \n",
    "        print('gathering data from: ', start_date)\n",
    "        \n",
    "        formatted_lookup_URL = lookup_URL.format(start_date.year,\n",
    "                                                 start_date.month,\n",
    "                                                 start_date.day)\n",
    "\n",
    "        driver.get(formatted_lookup_URL)\n",
    "        rows = WebDriverWait(driver, 60).until(EC.visibility_of_all_elements_located((By.XPATH, '//td[@class=\"mat-cell cdk-cell cdk-column-liquidPrecipitation mat-column-liquidPrecipitation ng-star-inserted\"]')))\n",
    "        for row in rows:\n",
    "            hour = timestamp.strftime('%H')\n",
    "            day = timestamp.strftime('%d')\n",
    "            prep = row.find_element_by_xpath('.//span[@class=\"wu-value wu-value-to\"]').text\n",
    "            # append new row to table\n",
    "            # 'dayhour' column will serve as column index to perform the Join\n",
    "            df_prep = df_prep.append(pd.DataFrame({\"dayhour\":[day+hour], 'precipitation':[prep]}),\n",
    "                                     ignore_index = True)\n",
    "            \n",
    "            timestamp += pd.Timedelta('1 hour')\n",
    "\n",
    "        start_date += timedelta(days=1)\n",
    "    return df_prep\n",
    "\n",
    "# GET INPUT DATA USING THE FUNCTIONS ABOVE: LocationIDs and Datetime info\n",
    "@st.cache\n",
    "def get_input_data(start_date, NoOfDays):\n",
    "    # get LocationIDs data frame\n",
    "    df_LocIds = get_LocationIDs()\n",
    "\n",
    "    # create datetime info and append LocationsIDs\n",
    "    dtInfo_and_LocID = datetimeInfo_and_LocID(df_LocIds,start_date,NoOfDays)\n",
    "\n",
    "    # get precipitation forecast\n",
    "    prep_forecast = scrape_data(date.today(), NoOfDays)\n",
    "\n",
    "    # merge both data frames\n",
    "    df_merged = dtInfo_and_LocID.merge(prep_forecast, on=\"dayhour\", how=\"left\")\n",
    "\n",
    "    # drop dayhour column\n",
    "    df_merged = df_merged.drop(['dayhour'], axis=1)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# GET OUPPUT DATA: get predictions, append to input_data and format it to be processed\n",
    "@st.cache\n",
    "def get_output_data(pickle_file, input_data):\n",
    "    import pickle\n",
    "\n",
    "    model = pickle.load(open(pickle_file,'rb'))\n",
    "\n",
    "    # get prediction, convert to integer and convert Array into DataFrame\n",
    "    model_predict = (model.predict(input_data)).astype(int)\n",
    "    df_predict = pd.DataFrame({'pickups':model_predict})\n",
    "\n",
    "    # join input_data with DataFrame\n",
    "    joined = input_data.join(df_predict)\n",
    "    \n",
    "    output_data = joined[['hour','dayofweek','LocationID','pickups']]\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "# GET DATA FRAME WITH SHAPE GEOMETRY INFO\n",
    "@st.cache\n",
    "def load_shape_data():\n",
    "    shape_data = gpd.read_file('../data/taxi_zones/taxi_zones.shp')\n",
    "\n",
    "    # filter Manhattan zones\n",
    "    shape_data = shape_data[shape_data['borough'] == 'Manhattan'].reset_index(drop=True)\n",
    "\n",
    "    shape_data = shape_data.drop(['borough'], axis=1)\n",
    "\n",
    "    #EPSG-Code of Web Mercador\n",
    "    shape_data.to_crs(epsg=3785, inplace=True)\n",
    "\n",
    "    # Simplify Shape of Zones (otherwise slow peformance of plot)\n",
    "    shape_data[\"geometry\"] = shape_data[\"geometry\"].simplify(100)\n",
    "\n",
    "    data = []\n",
    "    for zonename, LocationID, shape in shape_data[[\"zone\", \"LocationID\", \"geometry\"]].values:\n",
    "        #If shape is polygon, extract X and Y coordinates of boundary line:\n",
    "        if isinstance(shape, Polygon):\n",
    "            X, Y = shape.boundary.xy\n",
    "            X = [int(x) for x in X]\n",
    "            Y = [int(y) for y in Y]\n",
    "            data.append([LocationID, zonename, X, Y])\n",
    "\n",
    "        #If shape is Multipolygon, extract X and Y coordinates of each sub-Polygon:\n",
    "        if isinstance(shape, MultiPolygon):\n",
    "            for poly in shape:\n",
    "                X, Y = poly.boundary.xy\n",
    "                X = [int(x) for x in X]\n",
    "                Y = [int(y) for y in Y]\n",
    "                data.append([LocationID, zonename, X, Y])\n",
    "\n",
    "    #Create new DataFrame with X an Y coordinates separated:\n",
    "    shape_data = pd.DataFrame(data, columns=[\"LocationID\", \"ZoneName\", \"X\", \"Y\"])\n",
    "    return shape_data\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_taxis_data(output_data, shape_data):\n",
    "    df_to_visualize = shape_data.copy()\n",
    "    pickups = output_data.groupby(['hour','dayofweek','LocationID']).sum()\n",
    "    start_day = pd.unique(output_data['dayofweek']).min()\n",
    "    end_day = pd.unique(output_data['dayofweek']).max()\n",
    "\n",
    "    for hour in range(24):\n",
    "        for dayofweek in range(start_day,end_day+1,1):\n",
    "            # get pickups for this hour and weekday\n",
    "            p = pd.DataFrame(pickups.loc[(hour, dayofweek)]).reset_index()\n",
    "        \n",
    "            # add pickups to the Taxi Zones DataFrame       \n",
    "            df_to_visualize = pd.merge(df_to_visualize, p, on=\"LocationID\", how=\"left\").fillna(0)\n",
    "            # rename column as per day and hour\n",
    "            df_to_visualize.rename(columns={\"pickups\" : \"Passenger_%d_%d\"%(dayofweek, hour)}, inplace=True)\n",
    "\n",
    "    return df_to_visualize\n",
    "\n",
    "\n",
    "#############################   DEFINE FUNCTIONS END   #############################\n",
    "\n",
    "# DECLARE VARIABLES: start date, NoOfDays, pickle_file\n",
    "start_date = date.today() + timedelta(days=1) # start day is tomorrow\n",
    "NoOfDays = 3 # number of days for prediction\n",
    "pickle_file = './model_regGB.pickle'\n",
    "\n",
    "# RUN FUNCTIONS\n",
    "input_data = get_input_data(start_date, NoOfDays)\n",
    "\n",
    "output_data = get_output_data(pickle_file, input_data)\n",
    "\n",
    "shape_data = load_shape_data()\n",
    "\n",
    "df_to_visualize = load_taxis_data(output_data,shape_data)\n",
    "\n",
    "# INITIAL SET PAGE CONFIG\n",
    "page_title = 'Taxi Demand Predictor'\n",
    "layout='wide'\n",
    "initial_sidebar_state = 'expanded'\n",
    "\n",
    "# SHOW TITLE AND DESCRIPTION\n",
    "st.title(\"Manhattan Taxi Demand Predictor\")\n",
    "\"\"\"\n",
    "Hi! With this Machine Learning app you can get your taxi demand prediction in Manhattan for the next 3 days!\n",
    "\n",
    "Just choose day and hour from the side bar and hover the mouse over the map.\n",
    "\n",
    "It will tell you how many passenger are expected in that time frame.\n",
    "\"\"\"\n",
    "\n",
    "# SIDE BAR\n",
    "st.sidebar.text('Choose DAY and TIME')\n",
    "# add slider widget: Hours\n",
    "hour = st.sidebar.slider(\"Hour\",min_value=0, max_value=23, value=7, step=1)\n",
    "\n",
    "\n",
    "# add buttons widget: Dayofweek\n",
    "button1_day = date.today() + pd.Timedelta(days=1)\n",
    "button2_day = date.today() + pd.Timedelta(days=2)\n",
    "button3_day = date.today() + pd.Timedelta(days=3)\n",
    "    \n",
    "button1 = st.sidebar.button(str(button1_day) + \" (\"+button1_day.strftime(\"%A\") + \")\")\n",
    "button2 = st.sidebar.button(str(button2_day) + \" (\"+button2_day.strftime(\"%A\") + \")\")\n",
    "button3 = st.sidebar.button(str(button3_day) + \" (\"+button3_day.strftime(\"%A\") + \")\")\n",
    "weekday = button1_day.weekday()\n",
    "if button1:\n",
    "    weekday = button1_day.weekday()\n",
    "if button2:\n",
    "    weekday = button2_day.weekday()\n",
    "if button3:\n",
    "    weekday = button3_day.weekday()\n",
    "    \n",
    "\n",
    "# ColumnDataSource transforms the data into something that Bokeh and Java understand\n",
    "df_to_visualize[\"Passengers\"] = df_to_visualize[\"Passenger_\" + str(weekday) + \"_\" + str(hour)]\n",
    "\n",
    "source = ColumnDataSource(df_to_visualize)\n",
    "\n",
    "max_passengers_per_hour = df_to_visualize[filter(lambda x: \"Passenger_\" in x, df_to_visualize.columns)].max().max()\n",
    "\n",
    "color_mapper = LinearColorMapper(palette=palette[::-1], high=max_passengers_per_hour, low=0)\n",
    "\n",
    "\n",
    "##### Color Bar\n",
    "color_bar = ColorBar(color_mapper = color_mapper,\n",
    "                     ticker = BasicTicker(),\n",
    "                    label_standoff=8,\n",
    "                     location=(0,0),\n",
    "                     orientation='vertical')\n",
    "\n",
    "p = figure(title=\"New York Taxi Pickups\",\n",
    "           plot_width=450, plot_height=750,\n",
    "           toolbar_location=None,\n",
    "           tools='pan,wheel_zoom,box_zoom,reset,save')\n",
    "p.xaxis.visible = False\n",
    "p.yaxis.visible = False\n",
    "\n",
    "p.xgrid.grid_line_color = None\n",
    "p.ygrid.grid_line_color = None\n",
    "\n",
    "#Get rid of zoom on axes:\n",
    "for t in p.tools:\n",
    "    if type(t) == WheelZoomTool:\n",
    "        t.zoom_on_axis = False\n",
    "\n",
    "patches = p.patches(xs=\"X\", ys=\"Y\", source=source,fill_alpha=1,\n",
    "                  fill_color={'field': 'Passengers',\n",
    "                              'transform': color_mapper},\n",
    "                  line_color=\"black\", alpha=0.5)\n",
    "\n",
    "hovertool = HoverTool(tooltips=[('Zone:', \"@ZoneName\"),\n",
    "                                (\"Passengers:\", \"@Passengers\")])\n",
    "p.add_tools(hovertool)\n",
    "\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "st.bokeh_chart(p)\n",
    "\n",
    "'''\n",
    "**Ideas to improve**:\n",
    "\n",
    "- Show day, hour and zone for mÃ¡ximum and minimum value.\n",
    "- Show line chart with pickup evolution throughout the day, and make it interactive, highlighting the zone selected in the map\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LocationIDs():\n",
    "    # 1. Import Location and Borough columns form NY TAXI ZONES dataset\n",
    "    dfzones = pd.read_csv('../data/NY_taxi_zones.csv', sep=',',\n",
    "                          usecols=['LocationID', 'borough'])\n",
    "\n",
    "    # 2. Filter Manhattan zones\n",
    "    dfzones = dfzones[dfzones['borough']=='Manhattan']\\\n",
    "                    .drop(['borough'], axis=1)\\\n",
    "                    .sort_values(by='LocationID')\\\n",
    "                    .drop_duplicates('LocationID').reset_index(drop=True)    \n",
    "    return dfzones\n",
    "\n",
    "# CREATE DATETIME INFO AND APPEND LOCATION IDs\n",
    "\n",
    "def datetimeInfo_and_LocID(df_LocIds, start_date, NoOfDays):   \n",
    "\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    \n",
    "    # repeat LocationIDs. All of them... for each hour\n",
    "    location_id_col = pd.concat([df_LocIds]*24*NoOfDays).reset_index(drop=True)\n",
    "\n",
    "    # create data frame with range of days with hourly period\n",
    "    df_pred = pd.DataFrame()\n",
    "    dates = pd.date_range(start = start_date, end = start_date + timedelta(days=NoOfDays), freq = \"H\")\n",
    "    df_pred['datetime'] = dates\n",
    "    df_pred.drop([df_pred.shape[0]-1], inplace=True)\n",
    "\n",
    "    # Create new columns from datetime\n",
    "    df_pred['month'] = df_pred['datetime'].dt.month\n",
    "    df_pred['hour'] = df_pred['datetime'].dt.hour\n",
    "    # 'dayhour' will serve as index to perform the join\n",
    "    df_pred['dayhour'] = df_pred['datetime'].dt.strftime('%d%H')\n",
    "    df_pred['week'] = df_pred['datetime'].dt.week\n",
    "    df_pred['dayofweek'] = df_pred['datetime'].dt.dayofweek\n",
    "\n",
    "\n",
    "    # Create date time index calendar\n",
    "    drange = pd.date_range(start=str(start_date.year)+'-01-01', end=str(start_date.year)+'-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=drange.min(), end=drange.max())\n",
    "    \n",
    "    # 8.3 create new columns 'date' and 'isholiday'\n",
    "    df_pred['date'] = pd.to_datetime(df_pred['datetime'].dt.date)\n",
    "    df_pred['isholiday'] = df_pred['datetime'].isin(holidays).astype(int)\n",
    "    \n",
    "    # drop 'date' and 'datetime' column\n",
    "    df_pred.drop(['datetime'], axis=1, inplace=True)\n",
    "    df_pred.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "    # repeat rows. 67 rows per hour\n",
    "    df_pred = df_pred.iloc[np.arange(len(df_pred)).repeat(len(df_LocIds))].reset_index(drop=True)\n",
    "    #df_index = df_index.iloc[np.arange(len(df_index)).repeat(67)].reset_index(drop=True)\n",
    "\n",
    "    df_pred = df_pred.join(location_id_col)\n",
    "    \n",
    "    return df_pred\n",
    "\n",
    "# SCRAPE PRECIPITATION FORECAST FROM wunderground.com\n",
    "\n",
    "def scrape_data(today, days_in):\n",
    "    # Use .format(YYYY, M, D)\n",
    "    lookup_URL = 'https://www.wunderground.com/hourly/us/ny/new-york-city/date/{}-{}-{}.html'\n",
    "\n",
    "    options = webdriver.ChromeOptions();\n",
    "    options.add_argument('headless'); # to run chrome in the backbroung\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver.exe', options=options)\n",
    "\n",
    "    start_date = today + pd.Timedelta(days=1)\n",
    "    end_date = today + pd.Timedelta(days=days_in + 1)\n",
    "\n",
    "    df_prep = pd.DataFrame()\n",
    "\n",
    "    while start_date != end_date:\n",
    "        timestamp = pd.Timestamp(str(start_date)+' 00:00:00')\n",
    "        \n",
    "        print('gathering data from: ', start_date)\n",
    "        \n",
    "        formatted_lookup_URL = lookup_URL.format(start_date.year,\n",
    "                                                 start_date.month,\n",
    "                                                 start_date.day)\n",
    "\n",
    "        driver.get(formatted_lookup_URL)\n",
    "        rows = WebDriverWait(driver, 60).until(EC.visibility_of_all_elements_located((By.XPATH, '//td[@class=\"mat-cell cdk-cell cdk-column-liquidPrecipitation mat-column-liquidPrecipitation ng-star-inserted\"]')))\n",
    "        for row in rows:\n",
    "            hour = timestamp.strftime('%H')\n",
    "            day = timestamp.strftime('%d')\n",
    "            prep = row.find_element_by_xpath('.//span[@class=\"wu-value wu-value-to\"]').text\n",
    "            # append new row to table\n",
    "            # 'dayhour' column will serve as column index to perform the Join\n",
    "            df_prep = df_prep.append(pd.DataFrame({\"dayhour\":[day+hour], 'precipitation':[prep]}),\n",
    "                                     ignore_index = True)\n",
    "            \n",
    "            timestamp += pd.Timedelta('1 hour')\n",
    "\n",
    "        start_date += timedelta(days=1)\n",
    "    return df_prep\n",
    "\n",
    "# GET INPUT DATA USING THE FUNCTIONS ABOVE: LocationIDs and Datetime info\n",
    "\n",
    "def get_input_data(start_date, NoOfDays):\n",
    "    # get LocationIDs data frame\n",
    "    df_LocIds = get_LocationIDs()\n",
    "\n",
    "    # create datetime info and append LocationsIDs\n",
    "    dtInfo_and_LocID = datetimeInfo_and_LocID(df_LocIds,start_date,NoOfDays)\n",
    "\n",
    "    # get precipitation forecast\n",
    "    prep_forecast = scrape_data(date.today(), NoOfDays)\n",
    "\n",
    "    # merge both data frames\n",
    "    df_merged = dtInfo_and_LocID.merge(prep_forecast, on=\"dayhour\", how=\"left\")\n",
    "\n",
    "    # drop dayhour column\n",
    "    df_merged = df_merged.drop(['dayhour'], axis=1)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# GET OUPPUT DATA: get predictions, append to input_data and format it to be processed\n",
    "\n",
    "def get_output_data(pickle_file, input_data):\n",
    "    import pickle\n",
    "\n",
    "    model = pickle.load(open(pickle_file,'rb'))\n",
    "\n",
    "    # get prediction, convert to integer and convert Array into DataFrame\n",
    "    model_predict = (model.predict(input_data)).astype(int)\n",
    "    df_predict = pd.DataFrame({'pickups':model_predict})\n",
    "\n",
    "    # join input_data with DataFrame\n",
    "    joined = input_data.join(df_predict)\n",
    "    \n",
    "    output_data = joined[['hour','dayofweek','LocationID','pickups']]\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "# GET DATA FRAME WITH SHAPE GEOMETRY INFO\n",
    "\n",
    "def load_shape_data():\n",
    "    shape_data = gpd.read_file('../data/taxi_zones/taxi_zones.shp')\n",
    "\n",
    "    # filter Manhattan zones\n",
    "    shape_data = shape_data[shape_data['borough'] == 'Manhattan'].reset_index(drop=True)\n",
    "\n",
    "    shape_data = shape_data.drop(['borough'], axis=1)\n",
    "\n",
    "    #EPSG-Code of Web Mercador\n",
    "    shape_data.to_crs(epsg=3785, inplace=True)\n",
    "\n",
    "    # Simplify Shape of Zones (otherwise slow peformance of plot)\n",
    "    shape_data[\"geometry\"] = shape_data[\"geometry\"].simplify(100)\n",
    "\n",
    "    data = []\n",
    "    for zonename, LocationID, shape in shape_data[[\"zone\", \"LocationID\", \"geometry\"]].values:\n",
    "        #If shape is polygon, extract X and Y coordinates of boundary line:\n",
    "        if isinstance(shape, Polygon):\n",
    "            X, Y = shape.boundary.xy\n",
    "            X = [int(x) for x in X]\n",
    "            Y = [int(y) for y in Y]\n",
    "            data.append([LocationID, zonename, X, Y])\n",
    "\n",
    "        #If shape is Multipolygon, extract X and Y coordinates of each sub-Polygon:\n",
    "        if isinstance(shape, MultiPolygon):\n",
    "            for poly in shape:\n",
    "                X, Y = poly.boundary.xy\n",
    "                X = [int(x) for x in X]\n",
    "                Y = [int(y) for y in Y]\n",
    "                data.append([LocationID, zonename, X, Y])\n",
    "\n",
    "    #Create new DataFrame with X an Y coordinates separated:\n",
    "    shape_data = pd.DataFrame(data, columns=[\"LocationID\", \"ZoneName\", \"X\", \"Y\"])\n",
    "    return shape_data\n",
    "\n",
    "\n",
    "def load_taxis_data(output_data, shape_data):\n",
    "    df_to_visualize = shape_data.copy()\n",
    "    pickups = output_data.groupby(['hour','dayofweek','LocationID']).sum()\n",
    "    start_day = pd.unique(output_data['dayofweek']).min()\n",
    "    end_day = pd.unique(output_data['dayofweek']).max()\n",
    "\n",
    "    for hour in range(24):\n",
    "        for dayofweek in range(start_day,end_day+1,1):\n",
    "            # get pickups for this hour and weekday\n",
    "            p = pd.DataFrame(pickups.loc[(hour, dayofweek)]).reset_index()\n",
    "        \n",
    "            # add pickups to the Taxi Zones DataFrame       \n",
    "            df_to_visualize = pd.merge(df_to_visualize, p, on=\"LocationID\", how=\"left\").fillna(0)\n",
    "            # rename column as per day and hour\n",
    "            df_to_visualize.rename(columns={\"pickups\" : \"Passenger_%d_%d\"%(dayofweek, hour)}, inplace=True)\n",
    "\n",
    "    return df_to_visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering data from:  2020-08-25\n",
      "gathering data from:  2020-08-26\n",
      "gathering data from:  2020-08-27\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bokeh.io import output_notebook, output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool, Select, ColumnDataSource, WheelZoomTool, LogColorMapper, LinearColorMapper, ColorBar, BasicTicker\n",
    "from bokeh.palettes import Viridis256 as palette\n",
    "from bokeh.layouts import row\n",
    "\n",
    "# DECLARE VARIABLES: start date, NoOfDays, pickle_file\n",
    "start_date = date.today() + timedelta(days=1) # start day is tomorrow\n",
    "NoOfDays = 3 # number of days for prediction\n",
    "pickle_file = './model_regGB.pickle'\n",
    "\n",
    "input_data = get_input_data(start_date, NoOfDays)\n",
    "\n",
    "output_data = get_output_data(pickle_file, input_data)\n",
    "\n",
    "shape_data = load_shape_data()\n",
    "\n",
    "df_to_visualize = load_taxis_data(output_data,shape_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2020\n",
       "1     2020\n",
       "2     2020\n",
       "3     2020\n",
       "4     2020\n",
       "5     2020\n",
       "6     2020\n",
       "7     2020\n",
       "8     2020\n",
       "9     2020\n",
       "10    2020\n",
       "11    2020\n",
       "12    2020\n",
       "13    2020\n",
       "14    2020\n",
       "15    2020\n",
       "16    2020\n",
       "17    2020\n",
       "18    2020\n",
       "19    2020\n",
       "20    2020\n",
       "21    2020\n",
       "22    2020\n",
       "23    2020\n",
       "24    2020\n",
       "25    2020\n",
       "26    2020\n",
       "27    2020\n",
       "28    2020\n",
       "29    2020\n",
       "30    2020\n",
       "31    2020\n",
       "32    2020\n",
       "33    2020\n",
       "34    2020\n",
       "35    2020\n",
       "36    2020\n",
       "37    2020\n",
       "38    2020\n",
       "39    2020\n",
       "40    2020\n",
       "41    2020\n",
       "42    2020\n",
       "43    2020\n",
       "44    2020\n",
       "45    2020\n",
       "46    2020\n",
       "47    2020\n",
       "48    2020\n",
       "49    2020\n",
       "50    2020\n",
       "51    2020\n",
       "52    2020\n",
       "53    2020\n",
       "54    2020\n",
       "55    2020\n",
       "56    2020\n",
       "57    2020\n",
       "58    2020\n",
       "59    2020\n",
       "60    2020\n",
       "61    2020\n",
       "62    2020\n",
       "63    2020\n",
       "64    2020\n",
       "65    2020\n",
       "66    2020\n",
       "67    2020\n",
       "68    2020\n",
       "69    2020\n",
       "70    2020\n",
       "71    2020\n",
       "Name: datetime, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "dates = pd.date_range(start = start_date, end = start_date + timedelta(days=NoOfDays), freq = \"H\")\n",
    "df_pred['datetime'] = dates\n",
    "df_pred.drop([df_pred.shape[0]-1], inplace=True)\n",
    "\n",
    "  # Create new columns from datetime\n",
    "df_pred['month'] = df_pred['datetime'].dt.month\n",
    "df_pred['datetime'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = date.today() + timedelta(days=1)\n",
    "start_date."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
